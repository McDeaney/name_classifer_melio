{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melio Fullstack Data Scientist Technical Interview\n",
    "\n",
    "### Task 1: Building the classifier\n",
    "\n",
    "This is the main data science component of the technical assessment.\n",
    "\n",
    "Build a classifier to determine whether the name belongs to a `Person`, `Company`, or `University`:\n",
    "\n",
    "  - You can use any library you want.\n",
    "  - You can use a rule-based classification, a pre-built model/embedding, build a model yourself or a hybrid. \n",
    "  - Format:\n",
    "    - If you are building an ML solution, the training of your model can be in a Jupyter notebook. \n",
    "    - If you are not building an ML solution, you will have to embed your python code into the app.\n",
    "\n",
    "Note that the classifications are generated by the client's upstream system, but it is not always correct. \n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "  1. Give enough information on how to run your solution (i.e. python version, packages, requirements.txt, Dockerfile, etc.).\n",
    "  2. State all of your assumptions, if any.\n",
    "  3. There is no right or wrong answer, but give a clear reasoning on each step you took. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirty_name</th>\n",
       "      <th>dirty_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wright Pentlow</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS Sydney Hadebe</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROF. HENNIE VORSTER</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENRICA HAYTER</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teboho Ngema</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Irène Klaves</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aila Tenpenny</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OCÉANNE DAWIDOWITSCH</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lóng Mac Geffen</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>THABISO BLIGNAUT</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Emalee Le Strange</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lindiwe wright</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IMIBONO FUELS CAPITAL PTY LTD</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>IMIBONO FUELS PTY LTD</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IMIBONO FUELS</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MR Miss Bronwyn Kotze</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DR. Anson Dudderidge</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PROF. FREDERICK TURNER</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KATHERINA HAWKEY</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       dirty_name dirty_label\n",
       "0                  Wright Pentlow      Person\n",
       "1                MS Sydney Hadebe      Person\n",
       "2            PROF. HENNIE VORSTER      Person\n",
       "3                   ENRICA HAYTER      Person\n",
       "4                    Teboho Ngema      Person\n",
       "5                    Irène Klaves      Person\n",
       "6                   Aila Tenpenny      Person\n",
       "7            OCÉANNE DAWIDOWITSCH      Person\n",
       "8                 Lóng Mac Geffen      Person\n",
       "9                THABISO BLIGNAUT      Person\n",
       "10              Emalee Le Strange      Person\n",
       "11                 lindiwe wright      Person\n",
       "12  IMIBONO FUELS CAPITAL PTY LTD     Company\n",
       "13          IMIBONO FUELS PTY LTD     Company\n",
       "14                  IMIBONO FUELS     Company\n",
       "15          MR Miss Bronwyn Kotze      Person\n",
       "16           DR. Anson Dudderidge      Person\n",
       "17         PROF. FREDERICK TURNER     Company\n",
       "18               KATHERINA HAWKEY      Person"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/names_data_candidate.csv')\n",
    "df.head(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f522b5",
   "metadata": {},
   "source": [
    "- Thought process for the Task 1\n",
    "    1. Task 1 seems like Named entity recognition problem. \n",
    "        - I have used spacy before for a similar task.\n",
    "        - Other options include Flair and HuggingFace bert-base-NER.\n",
    "        - According to Papers with Code SOA is ACE an LSTM Transformer. The dataset used is CoNLL 2003 (English).\n",
    "        - Lets go with Spacy for a baseline since Ive used it before.\n",
    "        \n",
    "        - Lets go with a model approach we can add rules if neccessary.\n",
    "        - We just need to make sure the model is compatiable with the deployment.\n",
    "\n",
    "    2. Inspect Data\n",
    "        - Check for nulls, missing labels and or other anomolies.\n",
    "        - Check distibution of labels. \n",
    "    \n",
    "    3. Since labels are dirty maybe we can run through the model and check.\n",
    "\n",
    "    4. Prepare data for training, lets go 70% train 10% validation 20%.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9b0b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "dirty_name     0\n",
      "dirty_label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003585fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1a5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4520\n",
      "Unique labels: ['Person' 'Company' 'University']\n",
      "Label distribution:\n",
      "dirty_label\n",
      "Person        3690\n",
      "Company        732\n",
      "University      98\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Unique labels: {df['dirty_label'].unique()}\")\n",
    "print(f\"Label distribution:\\n{df['dirty_label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f9b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent label distribution:\n",
      "dirty_label\n",
      "Person        81.637168\n",
      "Company       16.194690\n",
      "University     2.168142\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percent label distribution:\\n{(df['dirty_label'].value_counts() / len(df) * 100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25373b5",
   "metadata": {},
   "source": [
    "- Only 2% of the data is Universities. The data is heavily imbalanced.\n",
    "* We need to come up with a strategy.\n",
    "* Thoughts:\n",
    "    - It will affect Accuracy as data is skewed toward labeling Person.\n",
    "    - Since data is exact label and not sentances augmenting on data level maybe difficult. Oversample minority class (duplicate)| Augment data\n",
    "    - On algorithm level can adjust class weights or have prediction thresholds. \n",
    "    - I could use LLM to add universities to data and manually balance dataset. Remove some Persons & add more univerties and companies.\n",
    "    - Stratify when doing splits especially validation.\n",
    "    - Use transfer learning:\n",
    "        - Spacy already trained on Person.\n",
    "        - So we can levereage that.\n",
    "        - We can check labels by running model through persons only.\n",
    "    - Lets think about this spacy also does orgnaisation and will classify a university as such. \n",
    "    - If we can have subcategories and default to company it may be a good first apporach. \n",
    "    - rule baded mixed with model.\n",
    "    - Spacy also does orgnaisation and will classify university as such\n",
    "    - A subgroup (company and universiy) for organisation class could work\n",
    "    - if not university then most likely a company\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27f2a9",
   "metadata": {},
   "source": [
    "#### Lets Validate our Person class. \n",
    "\n",
    "Knowing that spacy can classify entity and the labels are dirty lets see the accuracies and suggest changes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f396e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53337f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bar (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b7ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_person_labels(csv_path, column_name=\"dirty_name\", label_column=\"dirty_label\"):\n",
    "    \"\"\"\n",
    "    Validate PERSON entity labels by comparing with spaCy's pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to your CSV file\n",
    "        column_name: Column containing the text data\n",
    "        label_column: Column containing the entity labels\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with validation results\n",
    "    \"\"\"\n",
    "    # Load your CSV file\n",
    "    print(\"Loading data from CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Basic data exploration\n",
    "    print(f\"Dataset size: {len(df)} rows\")\n",
    "    print(f\"Label distribution:\\n{df[label_column].value_counts()}\")\n",
    "    \n",
    "    # Load spaCy model - using the medium model for better NER accuracy\n",
    "    print(\"Loading spaCy model...\")\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # You can use sm, md, or lg based on your needs\n",
    "    \n",
    "    # Create an empty list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each row\n",
    "    print(\"Processing entities...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[column_name]\n",
    "        original_label = row[label_column]\n",
    "        \n",
    "        # Process with spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Check if spaCy found any PERSON entities\n",
    "        person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "        spacy_found_person = len(person_entities) > 0\n",
    "        \n",
    "        # Determine if this is a PERSON according to your dataset\n",
    "        is_labeled_person = original_label.upper()  == \"PERSON\"\n",
    "        \n",
    "        # Check for agreement/disagreement\n",
    "        status = \"CORRECT\" if spacy_found_person == is_labeled_person else \"POTENTIAL_ERROR\"\n",
    "        \n",
    "        # Add additional info for potential errors\n",
    "        confidence = None\n",
    "        suggestion = None\n",
    "        \n",
    "        if status == \"POTENTIAL_ERROR\":\n",
    "            if is_labeled_person and not spacy_found_person:\n",
    "                # You labeled it PERSON, but spaCy didn't find a PERSON\n",
    "                suggestion = \"This might not be a PERSON\"\n",
    "                # Get all entities spaCy found, if any\n",
    "                other_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "                confidence = \"low\" if other_entities else \"medium\"\n",
    "            \n",
    "            elif not is_labeled_person and spacy_found_person:\n",
    "                # spaCy found PERSON but your label is different\n",
    "                person_texts = [ent.text for ent in person_entities]\n",
    "                suggestion = f\"This might be a PERSON: {', '.join(person_texts)}\"\n",
    "                # Check confidence based on overlap\n",
    "                confidence = \"high\" if any(text.find(ent.text) >= 0 for ent in person_entities) else \"medium\"\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"text\": text,\n",
    "            \"your_label\": original_label,\n",
    "            \"spacy_found_person\": spacy_found_person,\n",
    "            \"spacy_entities\": [(ent.text, ent.label_) for ent in doc.ents],\n",
    "            \"status\": status,\n",
    "            \"confidence\": confidence,\n",
    "            \"suggestion\": suggestion\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    total = len(results_df)\n",
    "    correct = len(results_df[results_df[\"status\"] == \"CORRECT\"])\n",
    "    potential_errors = len(results_df[results_df[\"status\"] == \"POTENTIAL_ERROR\"])\n",
    "    \n",
    "    print(\"\\n--- SUMMARY STATISTICS ---\")\n",
    "    print(f\"Total entries analyzed: {total}\")\n",
    "    print(f\"Correct labels: {correct} ({correct/total*100:.2f}%)\")\n",
    "    print(f\"Potential errors: {potential_errors} ({potential_errors/total*100:.2f}%)\")\n",
    "    \n",
    "    # Breakdown of error types\n",
    "    false_persons = len(results_df[(results_df[\"your_label\"].str.upper() == \"PERSON\") & (~results_df[\"spacy_found_person\"])])\n",
    "    missed_persons = len(results_df[(results_df[\"your_label\"].str.upper() != \"PERSON\") & (results_df[\"spacy_found_person\"])])\n",
    "    \n",
    "    print(\"\\n--- ERROR BREAKDOWN ---\")\n",
    "    print(f\"Potentially false PERSON labels: {false_persons}\")\n",
    "    print(f\"Potentially missed PERSON labels: {missed_persons}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Function to save the results for further analysis\n",
    "def save_validation_results(results_df, output_path=\"person_validation_results.csv\"):\n",
    "    \"\"\"Save validation results to CSV file\"\"\"\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    # Also save a filtered version with only potential errors for easier review\n",
    "    errors_df = results_df[results_df[\"status\"] == \"POTENTIAL_ERROR\"]\n",
    "    error_path = output_path.replace(\".csv\", \"_errors_only.csv\")\n",
    "    errors_df.to_csv(error_path, index=False)\n",
    "    print(f\"Potential errors saved to {error_path}\")\n",
    "    \n",
    "    return errors_df\n",
    "\n",
    "# Function to create a correction guide\n",
    "def create_correction_guide(errors_df, output_path=\"person_correction_suggestions.csv\"):\n",
    "    \"\"\"Create a simplified guide for manual corrections\"\"\"\n",
    "    \n",
    "    correction_guide = errors_df[[\"index\", \"text\", \"your_label\", \"spacy_entities\", \"suggestion\"]].copy()\n",
    "    \n",
    "    # Add a column for manual decisions\n",
    "    correction_guide[\"corrected_label\"] = \"\"\n",
    "    correction_guide[\"notes\"] = \"\"\n",
    "    \n",
    "    correction_guide.to_csv(output_path, index=False)\n",
    "    print(f\"Correction guide saved to {output_path}\")\n",
    "    \n",
    "    return correction_guide\n",
    "\n",
    "# Main execution function\n",
    "def main(csv_path):\n",
    "    \"\"\"Run the full validation process\"\"\"\n",
    "    # Validate person labels\n",
    "    results_df = validate_person_labels(csv_path)\n",
    "    \n",
    "    # Save results\n",
    "    errors_df = save_validation_results(results_df)\n",
    "    \n",
    "    # Create correction guide\n",
    "    create_correction_guide(errors_df)\n",
    "    \n",
    "    # Display sample errors for immediate review\n",
    "    print(\"\\n--- SAMPLE ERROR CASES ---\")\n",
    "    sample_size = min(10, len(errors_df))\n",
    "    for i, (_, row) in enumerate(errors_df.sample(sample_size).iterrows()):\n",
    "        print(f\"\\nCase {i+1}:\")\n",
    "        print(f\"Text: '{row['text']}'\")\n",
    "        print(f\"Your label: {row['your_label']}\")\n",
    "        print(f\"spaCy entities: {row['spacy_entities']}\")\n",
    "        print(f\"Suggestion: {row['suggestion']}\")\n",
    "    \n",
    "    print(\"\\n--- NEXT STEPS ---\")\n",
    "    print(\"1. Review the error cases in the generated files\")\n",
    "    print(\"2. Make corrections to your original dataset based on this analysis\")\n",
    "    print(\"3. Consider running this validation again after corrections\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eecc487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV...\n",
      "Dataset size: 4520 rows\n",
      "Label distribution:\n",
      "dirty_label\n",
      "Person        3690\n",
      "Company        732\n",
      "University      98\n",
      "Name: count, dtype: int64\n",
      "Loading spaCy model...\n",
      "Processing entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [03:30<00:00, 21.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SUMMARY STATISTICS ---\n",
      "Total entries analyzed: 4520\n",
      "Correct labels: 4024 (89.03%)\n",
      "Potential errors: 496 (10.97%)\n",
      "\n",
      "--- ERROR BREAKDOWN ---\n",
      "Potentially false PERSON labels: 235\n",
      "Potentially missed PERSON labels: 261\n",
      "Results saved to person_validation_results.csv\n",
      "Potential errors saved to person_validation_results_errors_only.csv\n",
      "Correction guide saved to person_correction_suggestions.csv\n",
      "\n",
      "--- SAMPLE ERROR CASES ---\n",
      "\n",
      "Case 1:\n",
      "Text: 'Adriaan Africa'\n",
      "Your label: Person\n",
      "spaCy entities: [('Adriaan Africa', 'LOC')]\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "Case 2:\n",
      "Text: 'cléopatre zannuto'\n",
      "Your label: Person\n",
      "spaCy entities: []\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "Case 3:\n",
      "Text: 'Mr. rev. brandon fredericks'\n",
      "Your label: Company\n",
      "spaCy entities: [('brandon fredericks', 'PERSON')]\n",
      "Suggestion: This might be a PERSON: brandon fredericks\n",
      "\n",
      "Case 4:\n",
      "Text: 'Browsecat  Realblab YODO Blognation Bond'\n",
      "Your label: Person\n",
      "spaCy entities: []\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "Case 5:\n",
      "Text: 'Mr Dr. Martha Schmidt'\n",
      "Your label: Company\n",
      "spaCy entities: [('Martha Schmidt', 'PERSON')]\n",
      "Suggestion: This might be a PERSON: Martha Schmidt\n",
      "\n",
      "Case 6:\n",
      "Text: 'TRACEE ERICSSEN'\n",
      "Your label: Person\n",
      "spaCy entities: []\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "Case 7:\n",
      "Text: 'Jaxnation  Mydo Skipfire Feedfire Corporation'\n",
      "Your label: Company\n",
      "spaCy entities: [('Jaxnation  Mydo', 'PERSON'), ('Skipfire Feedfire Corporation', 'ORG')]\n",
      "Suggestion: This might be a PERSON: Jaxnation  Mydo\n",
      "\n",
      "Case 8:\n",
      "Text: 'Sello Ngobeni'\n",
      "Your label: Company\n",
      "spaCy entities: [('Sello Ngobeni', 'PERSON')]\n",
      "Suggestion: This might be a PERSON: Sello Ngobeni\n",
      "\n",
      "Case 9:\n",
      "Text: 'miss vusi marais'\n",
      "Your label: Person\n",
      "spaCy entities: []\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "Case 10:\n",
      "Text: 'NKOSINATHI VAN TONDER'\n",
      "Your label: Person\n",
      "spaCy entities: []\n",
      "Suggestion: This might not be a PERSON\n",
      "\n",
      "--- NEXT STEPS ---\n",
      "1. Review the error cases in the generated files\n",
      "2. Make corrections to your original dataset based on this analysis\n",
      "3. Consider running this validation again after corrections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = main('data/names_data_candidate.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f1758",
   "metadata": {},
   "source": [
    "- Cleaning the data manually will take too long lets just drop the cases we are unsure of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adfe806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV...\n",
      "Dataset size: 496 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17,\n",
       " 23,\n",
       " 28,\n",
       " 44,\n",
       " 48,\n",
       " 54,\n",
       " 105,\n",
       " 108,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 146,\n",
       " 147,\n",
       " 155,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 182,\n",
       " 185,\n",
       " 198,\n",
       " 226,\n",
       " 230,\n",
       " 247,\n",
       " 255,\n",
       " 287,\n",
       " 290,\n",
       " 294,\n",
       " 301,\n",
       " 307,\n",
       " 349,\n",
       " 354,\n",
       " 356,\n",
       " 364,\n",
       " 367,\n",
       " 371,\n",
       " 380,\n",
       " 387,\n",
       " 390,\n",
       " 392,\n",
       " 407,\n",
       " 411,\n",
       " 436,\n",
       " 468,\n",
       " 469,\n",
       " 509,\n",
       " 514,\n",
       " 517,\n",
       " 518,\n",
       " 534,\n",
       " 535,\n",
       " 538,\n",
       " 558,\n",
       " 568,\n",
       " 569,\n",
       " 582,\n",
       " 587,\n",
       " 595,\n",
       " 610,\n",
       " 625,\n",
       " 637,\n",
       " 652,\n",
       " 657,\n",
       " 664,\n",
       " 672,\n",
       " 680,\n",
       " 692,\n",
       " 696,\n",
       " 723,\n",
       " 738,\n",
       " 748,\n",
       " 758,\n",
       " 762,\n",
       " 771,\n",
       " 773,\n",
       " 777,\n",
       " 781,\n",
       " 791,\n",
       " 798,\n",
       " 799,\n",
       " 807,\n",
       " 818,\n",
       " 847,\n",
       " 855,\n",
       " 858,\n",
       " 863,\n",
       " 864,\n",
       " 876,\n",
       " 884,\n",
       " 886,\n",
       " 936,\n",
       " 970,\n",
       " 979,\n",
       " 988,\n",
       " 990,\n",
       " 995,\n",
       " 1006,\n",
       " 1008,\n",
       " 1018,\n",
       " 1027,\n",
       " 1030,\n",
       " 1032,\n",
       " 1058,\n",
       " 1063,\n",
       " 1092,\n",
       " 1097,\n",
       " 1122,\n",
       " 1131,\n",
       " 1151,\n",
       " 1211,\n",
       " 1228,\n",
       " 1254,\n",
       " 1278,\n",
       " 1283,\n",
       " 1288,\n",
       " 1308,\n",
       " 1313,\n",
       " 1338,\n",
       " 1340,\n",
       " 1344,\n",
       " 1345,\n",
       " 1350,\n",
       " 1363,\n",
       " 1364,\n",
       " 1372,\n",
       " 1374,\n",
       " 1380,\n",
       " 1384,\n",
       " 1385,\n",
       " 1392,\n",
       " 1412,\n",
       " 1421,\n",
       " 1424,\n",
       " 1425,\n",
       " 1452,\n",
       " 1465,\n",
       " 1470,\n",
       " 1471,\n",
       " 1507,\n",
       " 1517,\n",
       " 1542,\n",
       " 1544,\n",
       " 1575,\n",
       " 1584,\n",
       " 1593,\n",
       " 1603,\n",
       " 1618,\n",
       " 1634,\n",
       " 1635,\n",
       " 1639,\n",
       " 1647,\n",
       " 1648,\n",
       " 1649,\n",
       " 1656,\n",
       " 1662,\n",
       " 1663,\n",
       " 1664,\n",
       " 1666,\n",
       " 1668,\n",
       " 1695,\n",
       " 1697,\n",
       " 1700,\n",
       " 1719,\n",
       " 1733,\n",
       " 1742,\n",
       " 1754,\n",
       " 1773,\n",
       " 1775,\n",
       " 1789,\n",
       " 1796,\n",
       " 1808,\n",
       " 1820,\n",
       " 1826,\n",
       " 1829,\n",
       " 1832,\n",
       " 1833,\n",
       " 1836,\n",
       " 1846,\n",
       " 1847,\n",
       " 1851,\n",
       " 1852,\n",
       " 1873,\n",
       " 1875,\n",
       " 1889,\n",
       " 1891,\n",
       " 1894,\n",
       " 1903,\n",
       " 1926,\n",
       " 1954,\n",
       " 1955,\n",
       " 1967,\n",
       " 1968,\n",
       " 1969,\n",
       " 1970,\n",
       " 1973,\n",
       " 1983,\n",
       " 1997,\n",
       " 2002,\n",
       " 2009,\n",
       " 2020,\n",
       " 2025,\n",
       " 2046,\n",
       " 2061,\n",
       " 2066,\n",
       " 2067,\n",
       " 2070,\n",
       " 2075,\n",
       " 2099,\n",
       " 2103,\n",
       " 2107,\n",
       " 2119,\n",
       " 2156,\n",
       " 2159,\n",
       " 2166,\n",
       " 2168,\n",
       " 2175,\n",
       " 2176,\n",
       " 2179,\n",
       " 2183,\n",
       " 2184,\n",
       " 2190,\n",
       " 2198,\n",
       " 2218,\n",
       " 2221,\n",
       " 2227,\n",
       " 2232,\n",
       " 2237,\n",
       " 2239,\n",
       " 2244,\n",
       " 2248,\n",
       " 2262,\n",
       " 2269,\n",
       " 2270,\n",
       " 2277,\n",
       " 2283,\n",
       " 2294,\n",
       " 2309,\n",
       " 2321,\n",
       " 2345,\n",
       " 2348,\n",
       " 2360,\n",
       " 2361,\n",
       " 2373,\n",
       " 2376,\n",
       " 2377,\n",
       " 2382,\n",
       " 2389,\n",
       " 2395,\n",
       " 2406,\n",
       " 2410,\n",
       " 2417,\n",
       " 2427,\n",
       " 2429,\n",
       " 2437,\n",
       " 2461,\n",
       " 2471,\n",
       " 2484,\n",
       " 2489,\n",
       " 2493,\n",
       " 2495,\n",
       " 2508,\n",
       " 2516,\n",
       " 2521,\n",
       " 2527,\n",
       " 2534,\n",
       " 2541,\n",
       " 2544,\n",
       " 2549,\n",
       " 2551,\n",
       " 2566,\n",
       " 2567,\n",
       " 2570,\n",
       " 2584,\n",
       " 2586,\n",
       " 2621,\n",
       " 2624,\n",
       " 2629,\n",
       " 2633,\n",
       " 2654,\n",
       " 2657,\n",
       " 2658,\n",
       " 2664,\n",
       " 2669,\n",
       " 2676,\n",
       " 2679,\n",
       " 2683,\n",
       " 2694,\n",
       " 2696,\n",
       " 2721,\n",
       " 2731,\n",
       " 2743,\n",
       " 2745,\n",
       " 2776,\n",
       " 2782,\n",
       " 2784,\n",
       " 2785,\n",
       " 2786,\n",
       " 2789,\n",
       " 2809,\n",
       " 2810,\n",
       " 2827,\n",
       " 2831,\n",
       " 2846,\n",
       " 2848,\n",
       " 2851,\n",
       " 2853,\n",
       " 2856,\n",
       " 2866,\n",
       " 2876,\n",
       " 2877,\n",
       " 2881,\n",
       " 2890,\n",
       " 2891,\n",
       " 2892,\n",
       " 2896,\n",
       " 2898,\n",
       " 2904,\n",
       " 2916,\n",
       " 2929,\n",
       " 2933,\n",
       " 2938,\n",
       " 2952,\n",
       " 2957,\n",
       " 2965,\n",
       " 2975,\n",
       " 2978,\n",
       " 2980,\n",
       " 2987,\n",
       " 3009,\n",
       " 3014,\n",
       " 3015,\n",
       " 3017,\n",
       " 3022,\n",
       " 3054,\n",
       " 3070,\n",
       " 3075,\n",
       " 3077,\n",
       " 3084,\n",
       " 3123,\n",
       " 3135,\n",
       " 3142,\n",
       " 3162,\n",
       " 3166,\n",
       " 3174,\n",
       " 3175,\n",
       " 3177,\n",
       " 3185,\n",
       " 3188,\n",
       " 3190,\n",
       " 3194,\n",
       " 3201,\n",
       " 3217,\n",
       " 3232,\n",
       " 3234,\n",
       " 3236,\n",
       " 3249,\n",
       " 3265,\n",
       " 3270,\n",
       " 3274,\n",
       " 3275,\n",
       " 3285,\n",
       " 3290,\n",
       " 3316,\n",
       " 3324,\n",
       " 3333,\n",
       " 3349,\n",
       " 3360,\n",
       " 3361,\n",
       " 3376,\n",
       " 3380,\n",
       " 3398,\n",
       " 3410,\n",
       " 3422,\n",
       " 3445,\n",
       " 3446,\n",
       " 3463,\n",
       " 3471,\n",
       " 3472,\n",
       " 3475,\n",
       " 3477,\n",
       " 3513,\n",
       " 3523,\n",
       " 3558,\n",
       " 3560,\n",
       " 3567,\n",
       " 3570,\n",
       " 3571,\n",
       " 3603,\n",
       " 3611,\n",
       " 3641,\n",
       " 3643,\n",
       " 3646,\n",
       " 3666,\n",
       " 3667,\n",
       " 3668,\n",
       " 3673,\n",
       " 3684,\n",
       " 3706,\n",
       " 3709,\n",
       " 3710,\n",
       " 3721,\n",
       " 3731,\n",
       " 3746,\n",
       " 3754,\n",
       " 3756,\n",
       " 3761,\n",
       " 3768,\n",
       " 3785,\n",
       " 3786,\n",
       " 3795,\n",
       " 3798,\n",
       " 3811,\n",
       " 3812,\n",
       " 3817,\n",
       " 3838,\n",
       " 3841,\n",
       " 3857,\n",
       " 3862,\n",
       " 3867,\n",
       " 3868,\n",
       " 3876,\n",
       " 3881,\n",
       " 3893,\n",
       " 3901,\n",
       " 3903,\n",
       " 3910,\n",
       " 3925,\n",
       " 3932,\n",
       " 3934,\n",
       " 3945,\n",
       " 3959,\n",
       " 3967,\n",
       " 4013,\n",
       " 4022,\n",
       " 4044,\n",
       " 4046,\n",
       " 4051,\n",
       " 4054,\n",
       " 4070,\n",
       " 4079,\n",
       " 4094,\n",
       " 4108,\n",
       " 4114,\n",
       " 4116,\n",
       " 4120,\n",
       " 4130,\n",
       " 4136,\n",
       " 4143,\n",
       " 4145,\n",
       " 4148,\n",
       " 4152,\n",
       " 4159,\n",
       " 4164,\n",
       " 4166,\n",
       " 4179,\n",
       " 4182,\n",
       " 4186,\n",
       " 4198,\n",
       " 4199,\n",
       " 4211,\n",
       " 4225,\n",
       " 4227,\n",
       " 4239,\n",
       " 4247,\n",
       " 4252,\n",
       " 4256,\n",
       " 4270,\n",
       " 4275,\n",
       " 4298,\n",
       " 4301,\n",
       " 4318,\n",
       " 4322,\n",
       " 4324,\n",
       " 4333,\n",
       " 4336,\n",
       " 4342,\n",
       " 4348,\n",
       " 4369,\n",
       " 4412,\n",
       " 4423,\n",
       " 4425,\n",
       " 4431,\n",
       " 4433,\n",
       " 4445,\n",
       " 4461,\n",
       " 4470,\n",
       " 4473,\n",
       " 4474,\n",
       " 4486,\n",
       " 4498,\n",
       " 4505,\n",
       " 4506,\n",
       " 4508,\n",
       " 4510,\n",
       " 4511,\n",
       " 4512,\n",
       " 4515]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data from CSV...\")\n",
    "errors_df = pd.read_csv('data/person_validation_results_errors_only.csv')\n",
    "\n",
    "# Basic data exploration\n",
    "print(f\"Dataset size: {len(errors_df)} rows\")\n",
    "errors_df[\"index\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa4d84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7ddd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(errors_df[\"index\"].to_list(),axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bbbe45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"clean_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb4eae",
   "metadata": {},
   "source": [
    "- Lets build the classifier (Thank you claude for your assistance)\n",
    "- Create own classifier by filtering out org and person from spacy entities.\n",
    "- add keywords and rules for classifying orgainsiation to university or company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33645f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EVALUATION RESULTS =====\n",
      "Main Entity Type Accuracy: 0.8735\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG       0.58      0.81      0.68       569\n",
      "      PERSON       0.98      0.88      0.93      3455\n",
      "     UNKNOWN       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87      4024\n",
      "   macro avg       0.52      0.56      0.54      4024\n",
      "weighted avg       0.92      0.87      0.89      4024\n",
      "\n",
      "\n",
      "Organization Subtype Accuracy: 0.3726\n",
      "\n",
      "Detailed Subtype Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        COMPANY       0.93      0.34      0.50       473\n",
      "     UNIVERSITY       0.78      0.52      0.62        96\n",
      "        UNKNOWN       0.00      0.00      0.00         0\n",
      "WRONG_MAIN_TYPE       0.00      0.00      0.00         0\n",
      "\n",
      "       accuracy                           0.37       569\n",
      "      macro avg       0.43      0.22      0.28       569\n",
      "   weighted avg       0.90      0.37      0.52       569\n",
      "\n",
      "\n",
      "Model saved to 'models/entity_classifier'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Add custom extension to store organization subtypes\n",
    "Doc.set_extension(\"org_subtypes\", default={}, force=True)\n",
    "\n",
    "@Language.factory(\"org_subclassifier\")\n",
    "class OrganizationSubclassifier:\n",
    "    \"\"\"A component that filters for PERSON/ORG entities and subclassifies organizations\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp, name):\n",
    "        self.name = name\n",
    "        \n",
    "        # Keywords for university classification\n",
    "        self.university_keywords = [\n",
    "            \"university\", \"college\", \"institute\", \"school\", \n",
    "            \"academy\", \"polytechnic\", \"conservatory\"\n",
    "        ]\n",
    "        \n",
    "        # Keywords for company classification\n",
    "        self.company_keywords = [\n",
    "            \"inc\", \"corp\", \"ltd\", \"limited\", \"llc\", \"company\", \n",
    "            \"technologies\", \"systems\", \"group\", \"industries\"\n",
    "        ]\n",
    "        \n",
    "        # Known entities\n",
    "        self.known_entities = {\n",
    "            \"mit\": \"UNIVERSITY\",\n",
    "            \"harvard\": \"UNIVERSITY\",\n",
    "            \"oxford\": \"UNIVERSITY\",\n",
    "            \"cambridge\": \"UNIVERSITY\",\n",
    "            \"apple\": \"COMPANY\",\n",
    "            \"google\": \"COMPANY\",\n",
    "            \"microsoft\": \"COMPANY\",\n",
    "            \"amazon\": \"COMPANY\"\n",
    "        }\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        # Filter entities to keep only PERSON and ORG\n",
    "        filtered_ents = []\n",
    "        \n",
    "        # If no entities were found but we have text, try to classify it\n",
    "        if not doc.ents and len(doc.text) > 0:\n",
    "            # Since we're dealing with single entities, try to classify the whole text\n",
    "            entity_text = doc.text.lower()\n",
    "            entity_type = self._guess_entity_type(entity_text)\n",
    "            \n",
    "            if entity_type in [\"PERSON\", \"ORG\"]:\n",
    "                # Create a span covering the whole text\n",
    "                span = doc.char_span(0, len(doc.text), label=entity_type)\n",
    "                if span:\n",
    "                    filtered_ents.append(span)\n",
    "        else:\n",
    "            # Filter existing entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "                    filtered_ents.append(ent)\n",
    "        \n",
    "        # Overwrite doc.ents with our filtered list\n",
    "        doc.ents = filtered_ents\n",
    "        \n",
    "        # Subclassify organizations\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                subtype = self._classify_organization(ent.text)\n",
    "                doc._.org_subtypes[ent.text] = subtype\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def _guess_entity_type(self, text):\n",
    "        \"\"\"Guess if an entity is a PERSON or ORG when spaCy NER doesn't detect it\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Check for known entities\n",
    "        for entity, subtype in self.known_entities.items():\n",
    "            if entity in text:\n",
    "                return \"ORG\"  # All our known entities are organizations\n",
    "        \n",
    "        # Check for organization keywords\n",
    "        if any(keyword in text for keyword in self.university_keywords + self.company_keywords):\n",
    "            return \"ORG\"\n",
    "        \n",
    "        # Check for person-like patterns (1-3 words, no special chars except ' and -)\n",
    "        words = text.split()\n",
    "        if (1 <= len(words) <= 3 and \n",
    "            all(word.isalpha() or \"'\" in word or \"-\" in word for word in words)):\n",
    "            return \"PERSON\"\n",
    "        \n",
    "        # Default to ORG \n",
    "        return \"ORG\"\n",
    "    \n",
    "    def _classify_organization(self, text):\n",
    "        \"\"\"Subclassify an organization as UNIVERSITY or COMPANY\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Check known entities first\n",
    "        for entity, subtype in self.known_entities.items():\n",
    "            if entity in text:\n",
    "                return subtype\n",
    "        \n",
    "        # Check for university indicators\n",
    "        if any(keyword in text for keyword in self.university_keywords) or \" of \" in text:\n",
    "            return \"UNIVERSITY\"\n",
    "        \n",
    "        # Default to COMPANY\n",
    "        return \"COMPANY\"\n",
    "\n",
    "def evaluate_entity_classifier(nlp, data):\n",
    "    \"\"\"Evaluate the entity classifier on test data\"\"\"\n",
    "    y_true_main = []  # Main entity type (PERSON/ORG)\n",
    "    y_pred_main = []\n",
    "    \n",
    "    y_true_sub = []   # Organization subtype (UNIVERSITY/COMPANY)\n",
    "    y_pred_sub = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        text = row[\"entity\"]\n",
    "        true_label = row[\"label\"].lower()\n",
    "        \n",
    "        # Map true labels\n",
    "        if \"person\" in true_label:\n",
    "            true_main = \"PERSON\"\n",
    "            true_sub = None\n",
    "        elif \"company\" in true_label:\n",
    "            true_main = \"ORG\"\n",
    "            true_sub = \"COMPANY\"\n",
    "        elif \"university\" in true_label:\n",
    "            true_main = \"ORG\"\n",
    "            true_sub = \"UNIVERSITY\"\n",
    "        else:\n",
    "            # Skip other entity types that aren't relevant\n",
    "            continue\n",
    "        \n",
    "        # Classify with our model\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Check if any entity was found\n",
    "        if doc.ents:\n",
    "            pred_main = doc.ents[0].label_\n",
    "            \n",
    "            # Get subtype for organizations\n",
    "            if pred_main == \"ORG\":\n",
    "                pred_sub = doc._.org_subtypes.get(text, \"UNKNOWN\")\n",
    "            else:\n",
    "                pred_sub = None\n",
    "        else:\n",
    "            # No entity detected\n",
    "            pred_main = \"UNKNOWN\"\n",
    "            pred_sub = None\n",
    "        \n",
    "        # Record results\n",
    "        y_true_main.append(true_main)\n",
    "        y_pred_main.append(pred_main)\n",
    "        \n",
    "        # Only evaluate subtypes for organizations\n",
    "        if true_main == \"ORG\" and true_sub:\n",
    "            y_true_sub.append(true_sub)\n",
    "            \n",
    "            # If main prediction is wrong, count subtype as wrong too\n",
    "            if pred_main == \"ORG\":\n",
    "                y_pred_sub.append(pred_sub)\n",
    "            else:\n",
    "                y_pred_sub.append(\"WRONG_MAIN_TYPE\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'main_accuracy': accuracy_score(y_true_main, y_pred_main),\n",
    "        'main_report': classification_report(y_true_main, y_pred_main, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if y_true_sub:\n",
    "        results['sub_accuracy'] = accuracy_score(y_true_sub, y_pred_sub)\n",
    "        results['sub_report'] = classification_report(y_true_sub, y_pred_sub, zero_division=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    df = pd.read_csv(\"clean_data.csv\")\n",
    "    df.columns = [\"dirty_name\", \"dirty_label\"]\n",
    "    df[\"entity\"] = df[\"dirty_name\"].str.strip()\n",
    "    df[\"label\"] = df[\"dirty_label\"].str.strip()\n",
    "    \n",
    "    # Load spaCy with NER\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    # Add our subclassifier after NER\n",
    "    nlp.add_pipe(\"org_subclassifier\", after=\"ner\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_entity_classifier(nlp, df)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"===== EVALUATION RESULTS =====\")\n",
    "    print(f\"Main Entity Type Accuracy: {results['main_accuracy']:.4f}\")\n",
    "    print(\"\\nDetailed Report:\")\n",
    "    print(results['main_report'])\n",
    "    \n",
    "    if 'sub_accuracy' in results:\n",
    "        print(f\"\\nOrganization Subtype Accuracy: {results['sub_accuracy']:.4f}\")\n",
    "        print(\"\\nDetailed Subtype Report:\")\n",
    "        print(results['sub_report'])\n",
    "    \n",
    "    # Save the model\n",
    "    nlp.to_disk(\"models/entity_classifier\")\n",
    "    print(\"\\nModel saved to 'models/entity_classifier'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48828761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom extension to store organization subtypes\n",
    "Doc.set_extension(\"org_subtypes\", default={}, force=True)\n",
    "\n",
    "@Language.factory(\"org_subclassifier\")\n",
    "class OrganizationSubclassifier:\n",
    "    \"\"\"A component that filters for PERSON/ORG entities and subclassifies organizations\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp, name):\n",
    "        self.name = name\n",
    "        \n",
    "        # Keywords for university classification\n",
    "        self.university_keywords = [\n",
    "            \"university\", \"college\", \"institute\", \"school\", \n",
    "            \"academy\", \"polytechnic\", \"conservatory\"\n",
    "        ]\n",
    "        \n",
    "        # Keywords for company classification\n",
    "        self.company_keywords = [\n",
    "            \"inc\", \"corp\", \"ltd\", \"limited\", \"llc\", \"company\", \n",
    "            \"technologies\", \"systems\", \"group\", \"industries\"\n",
    "        ]\n",
    "        \n",
    "        # Known entities\n",
    "        self.known_entities = {\n",
    "            \"mit\": \"UNIVERSITY\",\n",
    "            \"harvard\": \"UNIVERSITY\",\n",
    "            \"oxford\": \"UNIVERSITY\",\n",
    "            \"cambridge\": \"UNIVERSITY\",\n",
    "            \"apple\": \"COMPANY\",\n",
    "            \"google\": \"COMPANY\",\n",
    "            \"microsoft\": \"COMPANY\",\n",
    "            \"amazon\": \"COMPANY\"\n",
    "        }\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        # Filter entities to keep only PERSON and ORG\n",
    "        filtered_ents = []\n",
    "        \n",
    "        # If no entities were found but we have text, try to classify it\n",
    "        if not doc.ents and len(doc.text) > 0:\n",
    "            # Since we're dealing with single entities, try to classify the whole text\n",
    "            entity_text = doc.text.lower()\n",
    "            entity_type = self._guess_entity_type(entity_text)\n",
    "            \n",
    "            if entity_type in [\"PERSON\", \"ORG\"]:\n",
    "                # Create a span covering the whole text\n",
    "                span = doc.char_span(0, len(doc.text), label=entity_type)\n",
    "                if span:\n",
    "                    filtered_ents.append(span)\n",
    "        else:\n",
    "            # Filter existing entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "                    filtered_ents.append(ent)\n",
    "        \n",
    "        # Overwrite doc.ents with our filtered list\n",
    "        doc.ents = filtered_ents\n",
    "        \n",
    "        # Subclassify organizations\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                subtype = self._classify_organization(ent.text)\n",
    "                doc._.org_subtypes[ent.text] = subtype\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def _guess_entity_type(self, text):\n",
    "        \"\"\"Guess if an entity is a PERSON or ORG when spaCy NER doesn't detect it\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Check for known entities\n",
    "        for entity, subtype in self.known_entities.items():\n",
    "            if entity in text:\n",
    "                return \"ORG\"  # All our known entities are organizations\n",
    "        \n",
    "        # Check for organization keywords\n",
    "        if any(keyword in text for keyword in self.university_keywords + self.company_keywords):\n",
    "            return \"ORG\"\n",
    "        \n",
    "        # Check for person-like patterns (1-3 words, no special chars except ' and -)\n",
    "        words = text.split()\n",
    "        if (1 <= len(words) <= 3 and \n",
    "            all(word.isalpha() or \"'\" in word or \"-\" in word for word in words)):\n",
    "            return \"PERSON\"\n",
    "        \n",
    "        # Default to ORG \n",
    "        return \"ORG\"\n",
    "    \n",
    "    def _classify_organization(self, text):\n",
    "        \"\"\"Subclassify an organization as UNIVERSITY or COMPANY\"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Check known entities first\n",
    "        for entity, subtype in self.known_entities.items():\n",
    "            if entity in text:\n",
    "                return subtype\n",
    "        \n",
    "        # Check for university indicators\n",
    "        if any(keyword in text for keyword in self.university_keywords) or \" of \" in text:\n",
    "            return \"UNIVERSITY\"\n",
    "        \n",
    "        # Default to COMPANY\n",
    "        return \"COMPANY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1c55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d1304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"models/entity_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e0ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple Inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c71292",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5882c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doc.ents:\n",
    "    pred_main = doc.ents[0].label_\n",
    "            \n",
    "    # Get subtype for organizations\n",
    "    if pred_main == \"ORG\":\n",
    "        pred_sub = doc._.org_subtypes[text]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2672ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d40a330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMPANY'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3130b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1772945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
